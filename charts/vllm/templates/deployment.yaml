apiVersion: apps/v1
kind: Deployment
metadata:
    name: vllm-{{ .Values.model.name | lower | replace "." "" }}
    namespace: {{ .Release.Namespace }}
    labels:
      {{- include "vllm.selectorLabels" . | nindent 6 }}
spec:
    replicas: 1
    selector:
      matchLabels:
        app: vllm-{{ .Values.model.name | lower  | replace "." "" }}
    template:
      metadata:
        labels:
          app: vllm-{{ .Values.model.name | lower | replace "." "" }}
      spec:
        volumes:
        - name: cache-volume
          emptyDir: {}
        # TODO: Explore making this a PVC
        #   persistentVolumeClaim:
        #     claimName: vllm-{{ .Values.model.name | lower }}
        containers:
        - name: {{ .Chart.Name }}
          command: ["/bin/sh", "-c"]
          # TODO: Get other env vars included
          {{- if .Values.hfToken }}
          env:
          - name: HUGGING_FACE_HUB_TOKEN
            valueFrom:
              secretKeyRef:
                name: hf-token
                key: token
          {{- end }}
          image: "{{ .Values.image.name }}:{{ .Values.image.tag | default $.Chart.AppVersion }}"
          args: ["vllm serve {{ .Values.model.name }} --temperature {{ .Values.model.temperature }} --max-model-len {{ .Values.model.contextLength }}{{ range .Values.args }} {{ . }}{{ end }}"]
          ports:
          - containerPort: 8080
          resources:
            limits:
              cpu: "10"
              memory: "20G"
              nvidia.com/gpu: "1"
            requests:
              cpu: "2"
              memory: "6G"
              nvidia.com/gpu: "1"
          volumeMounts:
          - name: cache-volume
            mountPath: /root/.cache/huggingface
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 5